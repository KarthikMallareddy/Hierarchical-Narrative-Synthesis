# ğŸ§  Hierarchical Narrative Synthesis

> **Deep Generative AI Â· Latent Representation Learning Â· Hierarchical Reasoning**

A two-phase intelligent document analysis system that trains deep generative models offline, then processes heterogeneous user data â€” CSV, PDF, TXT, LOG â€” through a multi-layer reasoning pipeline to produce structured narrative reports.

---

## ğŸ¯ Motive

Traditional RAG (Retrieval-Augmented Generation) systems simply embed documents, search by similarity, and pass results to an LLM. This works for simple Q&A but fails for:

- **Heterogeneous data** â€” mixing financial CSVs, server logs, and research PDFs
- **Noisy real-world documents** â€” OCR errors, inconsistent formatting
- **Deep cross-source reasoning** â€” understanding how a server anomaly in a log relates to a revenue drop in a CSV
- **Structured, evidence-backed reports** â€” not just answers but auditable narratives

This system addresses all four by introducing **learned latent representations**, **cluster-aware retrieval**, and a **4-layer hierarchical reasoning pipeline** before synthesis.

---

## ğŸ›ï¸ Architecture â€” 2-Phase Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PHASE 1 â€” OFFLINE TRAINING             â”‚
â”‚                                                  â”‚
â”‚  Synthetic Corpus (2300 segments)                â”‚
â”‚       â†“                                          â”‚
â”‚  EmbeddingModel (all-MiniLM-L6-v2, frozen)      â”‚
â”‚       â†“  384-d vectors                            â”‚
â”‚  DenoisingAutoencoder  â† trained from scratch    â”‚
â”‚       â†“  noise-robust 384-d                       â”‚
â”‚  VariationalAutoencoder â† trained from scratch   â”‚
â”‚       â†“  64-d latent vectors                      â”‚
â”‚  K-Means Clustering (5 clusters)                 â”‚
â”‚       â†“                                          â”‚
â”‚  Artifacts saved to trained_models/              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PHASE 2 â€” ONLINE INFERENCE             â”‚
â”‚                                                  â”‚
â”‚  User uploads CSV / PDF / TXT / LOG              â”‚
â”‚       â†“  ingestion + chunking                    â”‚
â”‚  Projection Pipeline                             â”‚
â”‚  (Embed â†’ DAE â†’ VAE â†’ Cluster)                   â”‚
â”‚       â†“  indexed in FAISS                        â”‚
â”‚  Hierarchical Reasoning (4 layers)               â”‚
â”‚       â†“                                          â”‚
â”‚  Mistral-7B Narrative Synthesis                  â”‚
â”‚       â†“                                          â”‚
â”‚  Evaluation Metrics                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Semantic Embeddings** | `sentence-transformers` â€” `all-MiniLM-L6-v2` (384-d) |
| **Denoising Autoencoder** | `PyTorch` â€” custom encoder-decoder, MSE loss |
| **Variational Autoencoder** | `PyTorch` â€” reparameterization trick, KL divergence + MSE |
| **Clustering** | `scikit-learn` â€” K-Means (5 clusters) |
| **Vector Store** | `FAISS` â€” IndexFlatL2, exact L2 similarity search |
| **LLM** | `Mistral-7B-Instruct-v0.2` via HuggingFace Inference API |
| **PDF Parsing** | `pdfminer.six` |
| **CSV Processing** | `pandas` |
| **Frontend** | `Streamlit` |
| **Visualization** | `plotly`, `scikit-learn` PCA |

---

## ğŸ—‚ï¸ Project Structure

```
GenAI/
â”œâ”€â”€ train.py                    # Offline training orchestrator
â”œâ”€â”€ app.py                      # Streamlit web application
â”œâ”€â”€ env_config.txt              # API keys (gitignored)
â”œâ”€â”€ env_template.txt            # Template for new contributors
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models.py               # EmbeddingModel, DAE, VAE, LatentClusterer
â”‚   â”œâ”€â”€ ingestion.py            # File parsers + chunker
â”‚   â”œâ”€â”€ synthesis_engine.py     # Projection pipeline + FAISS VectorStore
â”‚   â”œâ”€â”€ reasoning.py            # 4-layer HierarchicalReasoner
â”‚   â”œâ”€â”€ evaluator.py            # Quality metrics
â”‚   â”œâ”€â”€ llm_wrapper.py          # HuggingFace / OpenAI API abstraction
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ data_loader.py      # Synthetic multi-domain corpus builder
â”‚
â””â”€â”€ trained_models/             # Saved artifacts (gitignored)
    â”œâ”€â”€ dae_model.pth
    â”œâ”€â”€ vae_model.pth
    â”œâ”€â”€ clusterer.pkl
    â”œâ”€â”€ training_embeddings.npy
    â”œâ”€â”€ latent_vectors.npy
    â””â”€â”€ training_metadata.pkl
```

---

## ğŸ§© How Each Component Works

### 1. Data Ingestion (`src/ingestion.py`)
Handles 4 file types:
- **CSV** â†’ loaded with `pandas`, converted to string via `df.to_string()`
- **PDF** â†’ text extracted with `pdfminer.six`
- **TXT / LOG** â†’ decoded as UTF-8

All outputs are chunked into **200-word segments** â€” uniform input for the embedding model.

### 2. Semantic Embedding (`src/models.py â†’ EmbeddingModel`)
Uses `all-MiniLM-L6-v2` from HuggingFace â€” a transformer pre-trained on 1 billion+ sentence pairs. Converts each text segment into a **384-dimensional vector** where semantically similar text is geometrically close. Weights are **frozen** (not fine-tuned).

### 3. Denoising Autoencoder (`src/models.py â†’ DenoisingAutoencoder`)
- **Architecture**: 384 â†’ 256 â†’ 128 â†’ 256 â†’ 384
- **Training**: Gaussian noise added to input (Ïƒ=0.3), model learns to reconstruct the clean original
- **Purpose**: Smooths inconsistencies from heterogeneous data (OCR noise, CSV formatting artifacts, log syntax), producing robust representations

### 4. Variational Autoencoder (`src/models.py â†’ VariationalAutoencoder`)
- **Architecture**: 384 â†’ 256 â†’ Î¼, Ïƒ (64-d each) â†’ reparameterize â†’ 64-d â†’ 256 â†’ 384
- **Loss**: MSE reconstruction + KL divergence (weighted 0.001)
- **Purpose**: Compresses 384-d to 64-d probabilistic latent space. The smooth, continuous latent space enables meaningful cluster discovery and anomaly detection

### 5. Latent Clustering (`src/models.py â†’ LatentClusterer`)
K-Means (k=5) fitted on the 64-d VAE latent vectors. Assigns every document to one of 5 structural topic clusters. Used during retrieval to boost documents from the same cluster as the query.

### 6. Vector Store (`src/synthesis_engine.py â†’ VectorStore`)
FAISS `IndexFlatL2` stores raw 384-d embeddings for fast nearest-neighbour search. Metadata (original text content) stored separately in a `.pkl` file. Indexed per session â€” reset on each new file upload.

### 7. Hierarchical Reasoning (`src/reasoning.py â†’ HierarchicalReasoner`)

| Layer | Method | What it does |
|-------|--------|-------------|
| 1 â€” Planning | `decompose_query()` | LLM breaks query into 2â€“4 focused sub-questions |
| 2 â€” Retrieval | `cluster_aware_retrieve()` | FAISS search + -0.3 distance bonus for same-cluster docs |
| 3 â€” Evidence | `link_evidence()` | Groups docs by source type: CSV, PDF, LOG, text |
| 4 â€” Validation | `validate_evidence()` | Score variance â†’ confidence; flags cross-source conflicts |

### 8. Narrative Synthesis (`src/synthesis_engine.py â†’ generate_narrative()`)
Constructs a structured prompt containing the query, retrieved evidence (organised by source), validation metadata (confidence, conflicts), then calls **Mistral-7B-Instruct-v0.2** via HuggingFace Inference API. Output is a Markdown report with:
- Executive Summary
- Key Findings
- Supporting Evidence
- Anomalies & Conflicts

### 9. Evaluation (`src/evaluator.py`)

| Metric | How |
|--------|-----|
| **VAE Confidence** | `1 - avg_reconstruction_loss / 10` |
| **Anomaly Likelihood** | Distance from nearest cluster centroid |
| **Evidence Coverage** | % of context docs referenced in narrative |
| **Faithfulness** | LLM auditor scores narrative vs source (0â€“1) |

---

## ğŸš€ How to Use

### Prerequisites
- Python 3.10+
- Anaconda (recommended)

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Configure API Key
Copy the template and add your HuggingFace token:
```bash
cp env_template.txt env_config.txt
```
Edit `env_config.txt`:
```
HUGGINGFACE_API_KEY=hf_your_actual_key_here
HUGGINGFACE_MODEL=mistralai/Mistral-7B-Instruct-v0.2
LLM_PROVIDER=huggingface
```
Get a free token at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).

### 3. Train the Models (One-Time)
```bash
python train.py
```
This takes ~2â€“5 minutes and saves all model artifacts to `trained_models/`.

### 4. Launch the App
```bash
streamlit run app.py
```
Open [http://localhost:8501](http://localhost:8501) in your browser.

### 5. Using the App

**Tab 1 â€” Data Processing:**
1. Upload your files (CSV, PDF, TXT, LOG) in the sidebar
2. Click **"ğŸš€ Process Files"**
3. The system runs the full projection pipeline and auto-generates a narrative report

**Tab 2 â€” Analysis & Report:**
1. Type a specific query (e.g. *"What are the critical server events?"*)
2. Click **"Generate Report"**
3. Expand "Reasoning Internals" to see sub-queries, evidence map, and confidence
4. Expand "Evaluation Metrics" for quality scores

**Tab 3 â€” Latent Space Explorer:**
- PCA 2D scatter plot of your data coloured by cluster
- Training loss curves for DAE and VAE

---

## ğŸ”‘ What Makes It Hierarchical

**1. Representation Hierarchy** â€” Data abstracted through 5 levels:
```
Raw Text â†’ 384-d Embedding â†’ Denoised 384-d â†’ 64-d Latent â†’ Cluster Label
```

**2. Reasoning Hierarchy** â€” 4 ordered processing layers:
```
Plan â†’ Retrieve â†’ Link Evidence â†’ Validate â†’ Synthesize
```

**3. Narrative Hierarchy** â€” LLM produces structured tiered report:
```
Executive Summary â†’ Key Findings â†’ Evidence â†’ Anomalies
```

---

## ğŸ§ª Sample Use Cases

| Data | Query | What You Get |
|------|-------|-------------|
| `sales_q1.csv` + `server_logs.log` | "Are server errors affecting revenue?" | Cross-source analysis linking downtime to sales dips |
| `research_paper.pdf` | "Summarise the methodology" | Section-aware extraction with evidence citations |
| Multiple CSVs | "Compare performance across quarters" | Trend analysis with anomaly flags |

---

## âš™ï¸ Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `LLM_PROVIDER` | `huggingface` | `huggingface` or `openai` |
| `HUGGINGFACE_MODEL` | `mistralai/Mistral-7B-Instruct-v0.2` | Any HF instruction model |
| `OPENAI_API_KEY` | â€” | Required if using OpenAI provider |

---

## ğŸ“„ License

MIT License â€” free to use, modify, and distribute.

---

*Built with PyTorch Â· HuggingFace Â· FAISS Â· Streamlit*
